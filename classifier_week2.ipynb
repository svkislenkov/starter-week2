{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL\n",
    "\n",
    "We're going to be building a model that can classify fruits. \n",
    "\n",
    "Steps to get there: \n",
    "1. Install Dependencies and Setup\n",
    "2. Perform some data exploration\n",
    "3. Load data in from kaggle \n",
    "4. Scale data to be in the correct format for our model\n",
    "5. Make seperate training, validation, and testing sets\n",
    "6. Build custom deep learning model (hint add multiple input, output, and hidden layers)\n",
    "7. Train model (hint hint use the fit() function)\n",
    "8. Plot performance (how well did we do?!?!?) \n",
    "9. Do some quick testing with our own image\n",
    "10. Save the model\n",
    "11. #DONE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have new and improved documentation at:\n",
    "\n",
    "https://docs.google.com/document/d/1Nbtx1lg2J6yfMFdQdcewOzdcUaUSZ7qScDBjyGNJVQE/edit?usp=sharing\n",
    "\n",
    "^^^ Do all of this stuff first before proceeding here! ^^^\n",
    "\n",
    "\n",
    "By the way, here are some useful definitions that you should know. \n",
    "\n",
    "Labels: targets - in this case, the fruit. \n",
    "\n",
    "Features/inputs: inputs - in this case, the image. \n",
    "\n",
    "Training data - the data we feed to our model. \n",
    "\n",
    "Validation data - the data we use to evaluate our model to see how well it trained.\n",
    "\n",
    "Testing data - the data we use to test our model to see how well it generalizes (i.e. how well it performs on recognizing a fruit.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/maryjojohnson/Library/Python/3.12/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/maryjojohnson/Library/Python/3.12/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##! ARE YOU A WINDOWS USER OR HAVE A GPU ON YOUR COMPUTER? UNCOMMENT THIS CELL!!!!!!!!!!\n",
    "# (ew)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus: \n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Check out our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok - So, the goal here is going to be to split up our training data in order to train our models \n",
    "\n",
    "Like we said in the doc, we need to split our data into training, validation, and testing sets. \n",
    "\n",
    "We're going to start by simply writing our directory and checking out what our data looks like. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change 'data' to the name of your training set directory\n",
    "# You should see a list of classes\n",
    "data_dir = 'data'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this cell below is doing is just checking out what our data actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpimg\u001b[39;00m\n\u001b[1;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m mpimg\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/fruits/fruits-360/Training/Apple Braeburn/100_100.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('../input/fruits/fruits-360/Training/Apple Braeburn/100_100.jpg')\n",
    "print(img.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool - so we see what our data looks like. Now we're going to load it into our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning model needs to be in the right shape and format in order to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This formats our data...\n",
    "# TODO: Ensure the image size is kept at (100, 100)\n",
    "data = tf.keras.utils.image_dataset_from_directory(data_dir, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each time we call this, it gives us a new set of data\n",
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 32 images per batch, 100x100, 3 channels (R, G, B)\n",
    "batch = data_iterator.next()\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real quick, what is this doing? \n",
    "\n",
    "This is taking our data and formatting it into a shape that our model can understand. \n",
    "\n",
    "We're going to scale our data down to a range between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Our tensorflow model works with values between 0 and 1.\n",
    "2. Our images give us pixel R, G, B values from 0-255.\n",
    "\n",
    "Thus, we need to scale our input data down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Pixel values range from 0-255. We want to scale x to range between 0-1.\n",
    "# x represents our data, and y represents our class. Therefore, we shouldn't worry about y\n",
    "\n",
    "# TODO: Uncomment + complete the following statement:\n",
    "# data = data.map(lambda x,y: (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will now give us an iterator with our SCALED data!\n",
    "scaled_iterator = data.as_numpy_iterator()\n",
    "batch = scaled_iterator.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once previous TODOs are complete, you should see 4 100x100 images here (of fruits, hopefully)\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Include Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now, it's your turn. Using the '+ Code' button, complete the same steps. Except this time, with the testing directory...\n",
    "\n",
    "**Note: for functionality, you'll only need to pattern-match some of the lines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Validation Set\n",
    "\n",
    "Now, lets set up our validation set. \n",
    "\n",
    "We're going to steal from our training set to form our validation set. This is not usually optimal, but should work for our purposes, especially since we've now shuffled our data when we pull from our Fruits Training directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.7)\n",
    "val_size = int(len(data) * 0.3)\n",
    "# Leave our test data alone\n",
    "\n",
    "# TODO: put in the name of your test_data here\n",
    "test_size = int(len(test_data))\n",
    "\n",
    "# TODO: Make sure train_size + val_size + test_size lines up with the total size of your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how we separate the training + validation data...\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = test_data.take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build Deep Learning Model\n",
    "This can get complicated - so if you made it this far, you're doing great! (We will get into this more the second week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers we're looking for: \n",
    "Activation, \n",
    "MaxPooling2D,\n",
    "Conv2D,\n",
    "Dropout,\n",
    "Flatten,\n",
    "Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add in all your layers here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to train our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of how many epochs you want to train for (hint hint what you will use for your validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the arguments for model.fit() You should now be able to train your model!\n",
    "hist = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show our loss + validation loss (should be decreasing)\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show our accuracy (should be increasing)\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Quick Test-Throw in our own image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our image into the model and see how well it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put an image of a fruit into the local directory, and replace 'pomelo.png' with the file name\n",
    "\n",
    "# The colors here will probably show up unexpected. This is because cv2 uses G, B, R instead of R, G, B...\n",
    "# Let's fix that in the next cell\n",
    "import cv2\n",
    "img = cv2.imread('pomelo.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image\n",
    "resize = tf.image.resize(img, (100, 100))\n",
    "\n",
    "# Convert GBR to RGB by reordering the channels\n",
    "resize_rgb = tf.reverse(resize, axis=[-1])\n",
    "\n",
    "# Display the corrected RGB image\n",
    "plt.imshow(resize_rgb.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line just wraps our image in another set of parenthesis\n",
    "# This allows the argmax call to work\n",
    "np.expand_dims(resize_rgb, 0).shape\n",
    "yhat = model.predict(np.expand_dims(resize_rgb/255, 0))\n",
    "\n",
    "# This takes the argmax (shows us our predicted class)\n",
    "predicted_class_index = np.argmax(yhat[0])\n",
    "print(predicted_class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want more than just the class number... we want its name so we can use it in the API!\n",
    "\n",
    "This is the (kind of) gross and inefficient way we found of doing it...\n",
    "Feel free to implement this differently if you like- perhaps by outputting the dictionary then hardcoding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace \"Test\" with the name of your testing or training directory\n",
    "\n",
    "# Goes thorugh your directory, sorts each name alphabetically.\n",
    "# Tensorflow will sort your directory alphabetically, so this match names to the previous cell's number\n",
    "directory_names = sorted([name for name in os.listdir(\"Test\") if os.path.isdir(os.path.join(\"Test\", name))])\n",
    "\n",
    "fruits_dict = {}\n",
    "\n",
    "for index, name in enumerate(directory_names):\n",
    "    fruits_dict.update({index: name})\n",
    "\n",
    "fruit = fruits_dict[predicted_class_index]\n",
    "\n",
    "print(fruit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Save the Model\n",
    "So we can later use it in our API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save the model and put it into a file called fruitclassifier.keras\n",
    "# After running this cell, you should have a black-box like predictor file!\n",
    "model.save('fruitclassifier.keras')\n",
    "new_model = load_model('fruitclassifier.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
